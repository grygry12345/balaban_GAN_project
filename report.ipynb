{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN project with Ä°brahim artworks dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import glob\n",
    "import imageio\n",
    "import math\n",
    "from IPython import display\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions and helper variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nicely formatted time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)\n",
    "\n",
    "# print progress bar\n",
    "def progress(i, size, status=\"Progress\"):\n",
    "    progress = i / size * 100\n",
    "    progress = round(progress, 2)\n",
    "    print(\"\\r{}: {}%\".format(status, progress), end=\"\")\n",
    "\n",
    "# find number of images in the folder\n",
    "num_img = len(os.listdir(\"balaban_resim_cropped\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arguments"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_DIR = \"balaban_resim_cropped\"  # directory of the images\n",
    "DATA_PATH = \"data\"  # path to save the dataset\n",
    "IMG_PATH = \"train_images\"  # path to save the generated images\n",
    "\n",
    "GENERATE_RES = 1  # Generation resolution factor (1=32, 2=64, 3=96, 4=128, etc.)\n",
    "GENERATE_SQUARE = 32 * GENERATE_RES  # rows/cols (should be square)\n",
    "\n",
    "IMAGE_CHANNELS = 3  # number of channels of the images (RGB = 3, Grayscale = 1)\n",
    "\n",
    "PREVIEW_ROWS = 4\n",
    "PREVIEW_COLS = 7\n",
    "PREVIEW_MARGIN = 16\n",
    "\n",
    "SEED_SIZE = 100"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Augmentation Arguments (sample boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_BOOST_SIZE = num_img * 50 # number of samples to boost = 46 * 100 = 4600\n",
    "ROTATION_RANGE = 15\n",
    "WIDTH_SHIFT_RANGE = 0.25\n",
    "HEIGHT_SHIFT_RANGE = 0.25\n",
    "ZOOM_RANGE = 0.4\n",
    "HORIZONTAL_FLIP = True\n",
    "VERTICAL_FLIP = True\n",
    "FILL_MODE = \"reflect\"\n",
    "BRIGHTNESS_RANGE = [0.5, 1.5]\n",
    "SHEAR_RANGE = 0.05"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "MEDIAN_FILTER = True\n",
    "\n",
    "GAUSIAN_FILTER = False\n",
    "\n",
    "BILITERAL_FILTER = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "EPOCHS = 1000\n",
    "BUFFER_SIZE = 60000\n",
    "LR_RATE = 1.5e-6"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_filter(img):\n",
    "    return cv2.medianBlur(img, ksize=3)\n",
    "\n",
    "def gausian_filter(img):\n",
    "    return cv2.GaussianBlur(img, ksize=(3, 3), sigmaX=0)\n",
    "\n",
    "def biliteral_filter(img):\n",
    "    return cv2.bilateralFilter(img, d=9, sigmaColor=75, sigmaSpace=75)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image loading, preprocessing and image augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples already boosted!\n",
      "Number of images boosted: 2300\n",
      "Number of actual images: 46\n",
      "Number of images loaded: 2346\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(IMG_PATH):\n",
    "    os.makedirs(IMG_PATH)\n",
    "\n",
    "# if IMG paths has images.npy file, load it\n",
    "\n",
    "if not os.path.exists(IMG_PATH + \"/images.npy\"):\n",
    "    \n",
    "    for filename in os.listdir(IMG_PATH):\n",
    "        os.remove(IMG_PATH + \"/\" + filename)\n",
    "\n",
    "    images = np.empty((0, GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS))\n",
    "    for filename in os.listdir(ART_DIR):\n",
    "        \n",
    "        img = tf.keras.utils.load_img(ART_DIR + \"/\" + filename)\n",
    "        resized_img = img.resize((GENERATE_SQUARE, GENERATE_SQUARE))\n",
    "        img_array = tf.keras.utils.img_to_array(resized_img)\n",
    "\n",
    "        if MEDIAN_FILTER:\n",
    "            img_array = median_filter(img_array)\n",
    "        elif GAUSIAN_FILTER:\n",
    "            img_array = gausian_filter(img_array)\n",
    "        elif BILITERAL_FILTER:\n",
    "            img_array = biliteral_filter(img_array)\n",
    "\n",
    "        if img is not None:\n",
    "            images = np.append(images, [img_array], axis=0)\n",
    "    \n",
    "\n",
    "    datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "        rotation_range=ROTATION_RANGE,\n",
    "        width_shift_range=WIDTH_SHIFT_RANGE,\n",
    "        height_shift_range=HEIGHT_SHIFT_RANGE,\n",
    "        horizontal_flip=HORIZONTAL_FLIP,\n",
    "        vertical_flip=VERTICAL_FLIP,\n",
    "        zoom_range=ZOOM_RANGE,\n",
    "        fill_mode=FILL_MODE,\n",
    "        brightness_range=BRIGHTNESS_RANGE,\n",
    "        shear_range=SHEAR_RANGE,\n",
    "    )\n",
    "\n",
    "\n",
    "    imageIterator = datagen.flow(np.array(images), batch_size=1)\n",
    "\n",
    "\n",
    "    for i in range(SAMPLE_BOOST_SIZE):\n",
    "        aumented_img = imageIterator.next()\n",
    "        images = np.append(images, aumented_img, axis=0)\n",
    "        \n",
    "        progress(i, SAMPLE_BOOST_SIZE, status=\"Boosting samples\")\n",
    "        \n",
    "\n",
    "    print(\"\\nNumber of images boosted:\", SAMPLE_BOOST_SIZE)\n",
    "    print(\"Number of actual images:\", num_img)\n",
    "    print(\"Number of images loaded:\", len(images))\n",
    "        \n",
    "    np.save(IMG_PATH + \"/images.npy\", images)\n",
    "    print(\"Images array saved!\")\n",
    "\n",
    "else:\n",
    "    print(\"Samples already boosted!\")\n",
    "\n",
    "    images = np.load(IMG_PATH + \"/images.npy\")\n",
    "    \n",
    "    print(\"Number of images boosted:\", SAMPLE_BOOST_SIZE)\n",
    "    print(\"Number of actual images:\", num_img)\n",
    "    print(\"Number of images loaded:\", len(images))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = np.array(images).reshape(-1, GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n",
    "training_data = training_data.astype(\"float32\")\n",
    "training_data = training_data / 127.5 - 1\n",
    "\n",
    "training_data = tf.data.Dataset.from_tensor_slices(training_data).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(seed_size, channels):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(4*4*256,activation=\"relu\",input_dim=seed_size))\n",
    "    model.add(tf.keras.layers.Reshape((4,4,256)))\n",
    "\n",
    "    model.add(tf.keras.layers.UpSampling2D())\n",
    "    model.add(tf.keras.layers.Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "\n",
    "    model.add(tf.keras.layers.UpSampling2D())\n",
    "    model.add(tf.keras.layers.Conv2D(256,kernel_size=3,padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "   \n",
    "    # Output resolution, additional upsampling\n",
    "    model.add(tf.keras.layers.UpSampling2D())\n",
    "    model.add(tf.keras.layers.Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "\n",
    "    if GENERATE_RES>1:\n",
    "      model.add(tf.keras.layers.UpSampling2D(size=(GENERATE_RES,GENERATE_RES)))\n",
    "      model.add(tf.keras.layers.Conv2D(128,kernel_size=3,padding=\"same\"))\n",
    "      model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "      model.add(tf.keras.layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final CNN layer\n",
    "    model.add(tf.keras.layers.Conv2D(channels,kernel_size=3,padding=\"same\"))\n",
    "    model.add(tf.keras.layers.Activation(\"tanh\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 4096)              413696    \n",
      "                                                                 \n",
      " reshape_3 (Reshape)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_11 (UpSamplin  (None, 8, 8, 256)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_54 (Conv2D)          (None, 8, 8, 256)         590080    \n",
      "                                                                 \n",
      " batch_normalization_47 (Bat  (None, 8, 8, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 8, 8, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d_12 (UpSamplin  (None, 16, 16, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_55 (Conv2D)          (None, 16, 16, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_48 (Bat  (None, 16, 16, 256)      1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 16, 16, 256)       0         \n",
      "                                                                 \n",
      " up_sampling2d_13 (UpSamplin  (None, 32, 32, 256)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_56 (Conv2D)          (None, 32, 32, 128)       295040    \n",
      "                                                                 \n",
      " batch_normalization_49 (Bat  (None, 32, 32, 128)      512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 32, 32, 128)       0         \n",
      "                                                                 \n",
      " conv2d_57 (Conv2D)          (None, 32, 32, 3)         3459      \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 32, 32, 3)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,894,915\n",
      "Trainable params: 1,893,635\n",
      "Non-trainable params: 1,280\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator = build_generator(SEED_SIZE, IMAGE_CHANNELS)\n",
    "\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test generator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x26d78986d60>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "noise = tf.random.normal([1, SEED_SIZE])\n",
    "generated_image = generator(noise, training=False)\n",
    "\n",
    "plt.imshow(generated_image[0, :, :, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create discriminator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(image_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=3, strides=2, input_shape=image_shape, \n",
    "                     padding=\"same\"))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(tf.keras.layers.ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(512, kernel_size=3, strides=1, padding=\"same\"))\n",
    "    model.add(tf.keras.layers.BatchNormalization(momentum=0.8))\n",
    "    model.add(tf.keras.layers.LeakyReLU(alpha=0.2))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_58 (Conv2D)          (None, 16, 16, 32)        896       \n",
      "                                                                 \n",
      " leaky_re_lu_38 (LeakyReLU)  (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_59 (Conv2D)          (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " zero_padding2d_4 (ZeroPaddi  (None, 9, 9, 64)         0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " batch_normalization_50 (Bat  (None, 9, 9, 64)         256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_39 (LeakyReLU)  (None, 9, 9, 64)          0         \n",
      "                                                                 \n",
      " conv2d_60 (Conv2D)          (None, 5, 5, 128)         73856     \n",
      "                                                                 \n",
      " batch_normalization_51 (Bat  (None, 5, 5, 128)        512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_40 (LeakyReLU)  (None, 5, 5, 128)         0         \n",
      "                                                                 \n",
      " conv2d_61 (Conv2D)          (None, 5, 5, 256)         295168    \n",
      "                                                                 \n",
      " batch_normalization_52 (Bat  (None, 5, 5, 256)        1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_41 (LeakyReLU)  (None, 5, 5, 256)         0         \n",
      "                                                                 \n",
      " conv2d_62 (Conv2D)          (None, 5, 5, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_53 (Bat  (None, 5, 5, 512)        2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " leaky_re_lu_42 (LeakyReLU)  (None, 5, 5, 512)         0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 12800)             0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 12801     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,585,217\n",
      "Trainable params: 1,583,297\n",
      "Non-trainable params: 1,920\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "image_shape = (GENERATE_SQUARE, GENERATE_SQUARE, IMAGE_CHANNELS)\n",
    "\n",
    "discriminator = build_discriminator(image_shape)\n",
    "\n",
    "discriminator.summary()\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test discriminator model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.49988165]], shape=(1, 1), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "decision = discriminator(generated_image)\n",
    "print (decision)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create image save helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_images(cnt,noise):\n",
    "  image_array = np.full(( \n",
    "      PREVIEW_MARGIN + (PREVIEW_ROWS * (GENERATE_SQUARE+PREVIEW_MARGIN)), \n",
    "      PREVIEW_MARGIN + (PREVIEW_COLS * (GENERATE_SQUARE+PREVIEW_MARGIN)), IMAGE_CHANNELS), \n",
    "      255, dtype=np.uint8)\n",
    "  \n",
    "  generated_images = generator.predict(noise)\n",
    "\n",
    "  generated_images = 0.5 * generated_images + 0.5\n",
    "\n",
    "  image_count = 0\n",
    "  for row in range(PREVIEW_ROWS):\n",
    "      for col in range(PREVIEW_COLS):\n",
    "        r = row * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        c = col * (GENERATE_SQUARE+16) + PREVIEW_MARGIN\n",
    "        image_array[r:r+GENERATE_SQUARE,c:c+GENERATE_SQUARE] \\\n",
    "            = generated_images[image_count] * 255\n",
    "        image_count += 1\n",
    "\n",
    "          \n",
    "  output_path = os.path.join(DATA_PATH,'output')\n",
    "  if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "  \n",
    "  filename = os.path.join(output_path,f\"train-{cnt}.png\")\n",
    "  im = Image.fromarray(image_array)\n",
    "  im.save(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def generator_loss(fake_output):\n",
    "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(LR_RATE,0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(LR_RATE,0.5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training step  (Semi-automatic GradientTape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images):\n",
    "  seed = tf.random.normal([BATCH_SIZE, SEED_SIZE])\n",
    "\n",
    "  with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "    generated_images = generator(seed, training=True)\n",
    "\n",
    "    real_output = discriminator(images, training=True)\n",
    "    fake_output = discriminator(generated_images, training=True)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "    \n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(\\\n",
    "        gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(\\\n",
    "        disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_generator, generator.trainable_variables))\n",
    "    \n",
    "    discriminator_optimizer.apply_gradients(zip(\n",
    "        gradients_of_discriminator, \n",
    "        discriminator.trainable_variables))\n",
    "  return gen_loss,disc_loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, epochs):\n",
    "  gen_losses = []\n",
    "  disc_losses = []\n",
    "  fixed_seed = np.random.normal(0, 1, (PREVIEW_ROWS * PREVIEW_COLS, \n",
    "                                       SEED_SIZE))\n",
    "  start = time.time()\n",
    "\n",
    "  for epoch in range(epochs):\n",
    "    epoch_start = time.time()\n",
    "\n",
    "    gen_loss_list = []\n",
    "    disc_loss_list = []\n",
    "\n",
    "    for image_batch in dataset:\n",
    "      t = train_step(image_batch)\n",
    "      gen_loss_list.append(t[0])\n",
    "      disc_loss_list.append(t[1])\n",
    "\n",
    "    g_loss = sum(gen_loss_list) / len(gen_loss_list)\n",
    "    d_loss = sum(disc_loss_list) / len(disc_loss_list)\n",
    "\n",
    "    epoch_elapsed = time.time()-epoch_start\n",
    "    print (f'Epoch {epoch+1}, gen loss={g_loss},disc loss={d_loss},'\\\n",
    "           f' {hms_string(epoch_elapsed)}')\n",
    "    save_images(epoch,fixed_seed)\n",
    "\n",
    "    # add generator loss and discriminator loss to lists\n",
    "    gen_losses.append(g_loss)\n",
    "    disc_losses.append(d_loss)\n",
    "\n",
    "\n",
    "  elapsed = time.time()-start\n",
    "  print (f'Training time: {hms_string(elapsed)}')\n",
    "\n",
    "  return gen_losses, disc_losses"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, gen loss=0.8656303882598877,disc loss=1.317534327507019, 0:00:05.25\n",
      "1/1 [==============================] - 0s 112ms/step\n",
      "Epoch 2, gen loss=1.169566035270691,disc loss=0.9309693574905396, 0:00:02.22\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "Epoch 3, gen loss=1.364325761795044,disc loss=0.7351148128509521, 0:00:02.23\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 4, gen loss=1.471777319908142,disc loss=0.6591827273368835, 0:00:02.26\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "Epoch 5, gen loss=1.6289865970611572,disc loss=0.5622722506523132, 0:00:02.28\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "Epoch 6, gen loss=1.8048793077468872,disc loss=0.4664594233036041, 0:00:02.38\n",
      "1/1 [==============================] - 0s 21ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[89], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m gen_losses, disc_losses \u001b[39m=\u001b[39m train(training_data, EPOCHS)\n",
      "Cell \u001b[1;32mIn[88], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(dataset, epochs)\u001b[0m\n\u001b[0;32m     12\u001b[0m disc_loss_list \u001b[39m=\u001b[39m []\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m image_batch \u001b[39min\u001b[39;00m dataset:\n\u001b[1;32m---> 15\u001b[0m   t \u001b[39m=\u001b[39m train_step(image_batch)\n\u001b[0;32m     16\u001b[0m   gen_loss_list\u001b[39m.\u001b[39mappend(t[\u001b[39m0\u001b[39m])\n\u001b[0;32m     17\u001b[0m   disc_loss_list\u001b[39m.\u001b[39mappend(t[\u001b[39m1\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateless_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "gen_losses, disc_losses = train(training_data, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(gen_losses, label='Generator Loss')\n",
    "plt.plot(disc_losses, label='Discriminator Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create GIF for training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anim_file = 'dcgan.gif'\n",
    "\n",
    "images_data = []\n",
    "for i in range(EPOCHS):\n",
    "    filename = DATA_PATH+'/output/train-' + str(i) + '.png'\n",
    "    images_data.append(imageio.imread(filename))\n",
    "imageio.mimsave(anim_file, images_data, fps=30)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "if IPython.version_info > (6,2,0,''):\n",
    "    display.Image(filename=anim_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "372d82335397ec2c4152a245936a15e7a3c12ebd658e8164f77465e79ac77f50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
